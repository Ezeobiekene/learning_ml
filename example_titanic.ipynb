# Re-import required libraries after kernel reset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import (
    BaggingClassifier, GradientBoostingClassifier,
    StackingClassifier, VotingClassifier, RandomForestClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score
import matplotlib.pyplot as plt

# Reload the Titanic dataset
df = pd.read_csv('/mnt/data/titanic.csv')

# Drop unnecessary columns
df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])

# Fill missing values
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Encode categorical features
df['Sex'] = LabelEncoder().fit_transform(df['Sex'])
df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])

# Feature engineering
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df.drop(['SibSp', 'Parch'], axis=1, inplace=True)

# Define features and target
X = df.drop('Survived', axis=1)
y = df['Survived']

# Split and scale data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define base models
log_reg = LogisticRegression(max_iter=1000)
rf = RandomForestClassifier()
svc = SVC(probability=True)
dt = DecisionTreeClassifier()

# Define ensemble models
bag_model = BaggingClassifier(base_estimator=dt, n_estimators=10, random_state=42)
boost_model = GradientBoostingClassifier()
stack_model = StackingClassifier(
    estimators=[('lr', log_reg), ('svc', svc)],
    final_estimator=rf,
    cv=5
)
vote_model = VotingClassifier(
    estimators=[('lr', log_reg), ('rf', rf), ('svc', svc)],
    voting='soft'
)

# Fit all models
bag_model.fit(X_train_scaled, y_train)
boost_model.fit(X_train_scaled, y_train)
stack_model.fit(X_train_scaled, y_train)
vote_model.fit(X_train_scaled, y_train)

# Predictions and probabilities
models = {
    'Bagging': bag_model,
    'Boosting': boost_model,
    'Stacking': stack_model,
    'Voting': vote_model
}

results = {}

# Plot setup
plt.figure(figsize=(10, 8))
for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled)[:, 1]

    # Confusion Matrix & Report
    cm = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

    # Store results
    results[name] = {
        "confusion_matrix": cm,
        "classification_report": report,
        "roc_auc": roc_auc
    }

# Finalize ROC plot
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curves for Ensemble Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

results
